{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 1: Install and Import Dependencies\n",
    "!pip install -q transformers datasets torch sentencepiece accelerate evaluate huggingface_hub\n",
    "!pip install -q wandb  # for experiment tracking\n",
    "\n",
    "import os\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    Seq2SeqTrainingArguments,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import wandb\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 145 examples [00:00, 1092.49 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset structure: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['instruction', 'input', 'output'],\n",
      "        num_rows: 145\n",
      "    })\n",
      "})\n",
      "\n",
      "Sample example: {'instruction': 'What is the current price of Bitcoin?', 'input': 'price=45000', 'output': 'The price of Bitcoin is 45000 USD'}\n",
      "\n",
      "Training examples: 130\n",
      "Validation examples: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load and Prepare Dataset\n",
    "# Load dataset from your Hugging Face repository\n",
    "dataset = load_dataset(\"Mozes721/stock-crypto-weather-dataset\", data_files=\"fine_tuning_data.json\")\n",
    "\n",
    "# Print dataset info\n",
    "print(\"Dataset structure:\", dataset)\n",
    "print(\"\\nSample example:\", dataset['train'][0])\n",
    "\n",
    "# Split dataset into train and validation\n",
    "train_val = dataset['train'].train_test_split(test_size=0.1)\n",
    "train_dataset = train_val['train']\n",
    "val_dataset = train_val['test']\n",
    "\n",
    "print(f\"\\nTraining examples: {len(train_dataset)}\")\n",
    "print(f\"Validation examples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: google/flan-t5-small\n",
      "Model parameters: 76,961,152\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Initialize Model and Tokenizer\n",
    "model_name = \"google/flan-t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Print model info\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 130/130 [00:00<00:00, 3847.50 examples/s]\n",
      "Map: 100%|██████████| 15/15 [00:00<00:00, 3999.91 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input: How much does Ethereum cost?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "Sample tokenized target: Current Ethereum sits at 2890 USD</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Data Preprocessing\n",
    "def preprocess_function(examples):\n",
    "    # Tokenize inputs\n",
    "    inputs = examples['instruction']\n",
    "    targets = examples['output']\n",
    "    \n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    \n",
    "    # Tokenize targets\n",
    "    labels = tokenizer(\n",
    "        targets,\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Apply preprocessing\n",
    "tokenized_train = train_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "\n",
    "tokenized_val = val_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=val_dataset.column_names\n",
    ")\n",
    "\n",
    "print(\"Sample tokenized input:\", tokenizer.decode(tokenized_train[0]['input_ids']))\n",
    "print(\"Sample tokenized target:\", tokenizer.decode(tokenized_train[0]['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▃▁</td></tr><tr><td>eval/runtime</td><td>█▁▄</td></tr><tr><td>eval/samples_per_second</td><td>▁█▅</td></tr><tr><td>eval/steps_per_second</td><td>▁█▅</td></tr><tr><td>train/epoch</td><td>▁▂▃▃▃▄▅▅▆▇▇██</td></tr><tr><td>train/global_step</td><td>▁▂▃▃▃▄▅▅▆▇▇██</td></tr><tr><td>train/grad_norm</td><td>▅▇█▅▆▄▄▁▂</td></tr><tr><td>train/learning_rate</td><td>█▇▆▅▅▄▃▂▁</td></tr><tr><td>train/loss</td><td>█▆▅▄▃▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>23.44195</td></tr><tr><td>eval/runtime</td><td>0.2766</td></tr><tr><td>eval/samples_per_second</td><td>54.224</td></tr><tr><td>eval/steps_per_second</td><td>14.46</td></tr><tr><td>total_flos</td><td>17984906330112.0</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>train/global_step</td><td>99</td></tr><tr><td>train/grad_norm</td><td>84.65435</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>25.8229</td></tr><tr><td>train_loss</td><td>30.52334</td></tr><tr><td>train_runtime</td><td>41.13</td></tr><tr><td>train_samples_per_second</td><td>9.409</td></tr><tr><td>train_steps_per_second</td><td>2.407</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">flan-t5-small</strong> at: <a href='https://wandb.ai/mozesthegreat-catts/crypto-stock-weather-agent/runs/7k57if4s' target=\"_blank\">https://wandb.ai/mozesthegreat-catts/crypto-stock-weather-agent/runs/7k57if4s</a><br> View project at: <a href='https://wandb.ai/mozesthegreat-catts/crypto-stock-weather-agent' target=\"_blank\">https://wandb.ai/mozesthegreat-catts/crypto-stock-weather-agent</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250620_134840-7k57if4s/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/richardtaujenis/rtaujenis/personal_projects/LlamaRAG/jupyter/wandb/run-20250620_135604-kccljlui</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mozesthegreat-catts/crypto-stock-weather-agent/runs/kccljlui' target=\"_blank\">flan-t5-small</a></strong> to <a href='https://wandb.ai/mozesthegreat-catts/crypto-stock-weather-agent' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mozesthegreat-catts/crypto-stock-weather-agent' target=\"_blank\">https://wandb.ai/mozesthegreat-catts/crypto-stock-weather-agent</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mozesthegreat-catts/crypto-stock-weather-agent/runs/kccljlui' target=\"_blank\">https://wandb.ai/mozesthegreat-catts/crypto-stock-weather-agent/runs/kccljlui</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/mozesthegreat-catts/crypto-stock-weather-agent/runs/kccljlui?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x179422d50>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install -q nbformat\n",
    "# Training Arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "\n",
    "    eval_strategy=\"epoch\",  # Changed from evaluation_strategy\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=\"Mozes721/crypto-stock-weather-agent\",\n",
    "    hub_token=os.getenv(\"HF_ACCESS_TOKEN\"),\n",
    "    report_to=\"wandb\"\n",
    ")\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.init(\n",
    "    project=\"crypto-stock-weather-agent\",\n",
    "    name=\"flan-t5-small\",\n",
    "    config={\n",
    "        \"model_name\": \"flan-t5-small-finetuned\",\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"batch_size\": 4,\n",
    "        \"epochs\": 3,\n",
    "        \"weight_decay\": 0.01\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vk/476vpzr57lgbvz8vg4dbmtfc0000gn/T/ipykernel_69208/4171557761.py:10: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "/Users/richardtaujenis/rtaujenis/personal_projects/LlamaRAG/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='99' max='99' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [99/99 00:33, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>32.846100</td>\n",
       "      <td>33.556351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>28.503600</td>\n",
       "      <td>26.897081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>25.789700</td>\n",
       "      <td>24.864452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richardtaujenis/rtaujenis/personal_projects/LlamaRAG/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/richardtaujenis/rtaujenis/personal_projects/LlamaRAG/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Mozes721/crypto-stock-weather-agent/commit/9cf88cbc400eac7ff11535f684c82b4093cf603b', commit_message='End of training', commit_description='', oid='9cf88cbc400eac7ff11535f684c82b4093cf603b', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Mozes721/crypto-stock-weather-agent', endpoint='https://huggingface.co', repo_type='model', repo_id='Mozes721/crypto-stock-weather-agent'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the trainer\n",
    "\n",
    "from transformers import (\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(\"./results/flan-t5-small-finetuned\")\n",
    "\n",
    "# Push to Hub (this is automatic because we set push_to_hub=True in training_args)\n",
    "# But we can also do it manually:\n",
    "trainer.push_to_hub()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the model with different queries:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the current price of Bitcoin? | Values: price=45000\n",
      "Model Response: Bitcoin is a cryptocurrency that has a price of $45000\n",
      "---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How's the weather in Riga? | Values: temp=15 weather=partly cloudy wind_speed=4.8\n",
      "Model Response: Temperatures in Riga are 4.8 degrees Fahrenheit.\n",
      "---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Could you tell me the weather in Warsaw | Values: temp=20 weather=partly cloudy wind_speed=2.1\n",
      "Model Response: Weather in Warsaw is moderately cloudy.\n",
      "---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What's the current price of Tesla stock? | Values: price=250.75\n",
      "Model Response: The price of a Tesla stock is 250.75\n",
      "---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Tell me the price of Ethereum | Values: price=2800\n",
      "Model Response: Ethereum is a cryptocurrency with a price of 2800.\n",
      "---\n",
      "\n",
      "Query: What's the weather like in Tokyo? | Values: temp=22 weather=clear sky wind_speed=5.2\n",
      "Model Response: Tokyo has a temperature of 22 degrees Fahrenheit and a wind speed of 5.2 degrees Fahrenheit.\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the fine-tuned model\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the fine-tuned model from Hugging Face Hub\n",
    "model_id = \"Mozes721/crypto-stock-weather-agent\"\n",
    "generator = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model_id,\n",
    "    max_length=256,  # Longer to accommodate full templates\n",
    "    num_return_sequences=1,\n",
    "    temperature=0.1,  # More consistent outputs\n",
    "    do_sample=False  # More deterministic\n",
    ")\n",
    "\n",
    "# Test examples with values included in the instruction\n",
    "test_queries = [\n",
    "    {\n",
    "        \"instruction\": \"What is the current price of Bitcoin?\",\n",
    "        \"input\": \"price=45000\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"How's the weather in Riga?\",\n",
    "        \"input\": \"temp=15 weather=partly cloudy wind_speed=4.8\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Could you tell me the weather in Warsaw\",\n",
    "        \"input\": \"temp=20 weather=partly cloudy wind_speed=2.1\",\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What's the current price of Tesla stock?\",\n",
    "        \"input\": \"price=250.75\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Tell me the price of Ethereum\",\n",
    "        \"input\": \"price=2800\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What's the weather like in Tokyo?\",\n",
    "        \"input\": \"temp=22 weather=clear sky wind_speed=5.2\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Testing the model with different queries:\\n\")\n",
    "for query in test_queries:\n",
    "    # Combine instruction and input for the model\n",
    "    full_prompt = f\"{query['instruction']} | Values: {query['input']}\"\n",
    "    response = generator(full_prompt)\n",
    "    generated_text = response[0]['generated_text']\n",
    "    print(f\"Query: {full_prompt}\")\n",
    "    print(f\"Model Response: {generated_text}\")\n",
    "    print(\"---\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m formatted_query = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstruction\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_values\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Now, pass this correctly formatted string to your pipeline\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m response = \u001b[43mgenerator\u001b[49m(formatted_query)\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mQuery: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mformatted_query\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel Response: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse[\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mgenerated_text\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'generator' is not defined"
     ]
    }
   ],
   "source": [
    "# The instruction part of your query\n",
    "instruction = \"Could you tell me the weather in Warsaw\"\n",
    "\n",
    "# The values part\n",
    "input_values = \"temp=20 weather=partly cloudy wind_speed=2.1\"\n",
    "\n",
    "# Combine them EXACTLY as the model was trained\n",
    "# (A simple space is the most common way)\n",
    "formatted_query = f\"{instruction} {input_values}\"\n",
    "\n",
    "# Now, pass this correctly formatted string to your pipeline\n",
    "response = generator(formatted_query)\n",
    "\n",
    "print(f\"Query: {formatted_query}\")\n",
    "print(f\"Model Response: {response[0]['generated_text']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
